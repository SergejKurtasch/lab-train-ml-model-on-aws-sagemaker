{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heart Disease Prediction with SageMaker\n",
        "\n",
        "This notebook demonstrates a complete pipeline for training and deploying a machine learning model for heart disease prediction using AWS SageMaker.\n",
        "\n",
        "## Objectives:\n",
        "1. Prepare heart disease data\n",
        "2. Train AdaBoostClassifier model in SageMaker\n",
        "3. Deploy the model as an endpoint\n",
        "4. Test the model on new data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies and Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "%pip install scikit-learn pandas numpy sagemaker boto3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import sagemaker\n",
        "import boto3\n",
        "from sagemaker.sklearn.estimator import SKLearn\n",
        "from sagemaker.serializers import CSVSerializer\n",
        "from sagemaker.deserializers import JSONDeserializer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "Load and prepare heart disease data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load heart disease data from data folder\n",
        "try:\n",
        "    df = pd.read_csv('data/heart.csv')\n",
        "    print(\"Data loaded from data/heart.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"data/heart.csv file not found. Trying alternative paths...\")\n",
        "    try:\n",
        "        df = pd.read_csv('../data/heart.csv')\n",
        "        print(\"Data loaded from ../data/heart.csv\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Heart data not found in data folder. Creating synthetic data...\")\n",
        "        # Create synthetic data for demonstration\n",
        "        np.random.seed(42)\n",
        "        n_samples = 1000\n",
        "        \n",
        "        df = pd.DataFrame({\n",
        "            'age': np.random.randint(29, 80, n_samples),\n",
        "            'sex': np.random.randint(0, 2, n_samples),\n",
        "            'cp': np.random.randint(0, 4, n_samples),\n",
        "            'trestbps': np.random.randint(94, 201, n_samples),\n",
        "            'chol': np.random.randint(126, 565, n_samples),\n",
        "            'fbs': np.random.randint(0, 2, n_samples),\n",
        "            'restecg': np.random.randint(0, 3, n_samples),\n",
        "            'thalach': np.random.randint(71, 203, n_samples),\n",
        "            'exang': np.random.randint(0, 2, n_samples),\n",
        "            'oldpeak': np.random.uniform(0, 6.2, n_samples),\n",
        "            'slope': np.random.randint(0, 3, n_samples),\n",
        "            'ca': np.random.randint(0, 4, n_samples),\n",
        "            'thal': np.random.randint(0, 3, n_samples),\n",
        "            'target': np.random.randint(0, 2, n_samples)\n",
        "        })\n",
        "        \n",
        "        # Save synthetic data\n",
        "        df.to_csv('heart.csv', index=False)\n",
        "        print(\"Synthetic data saved to heart.csv\")\n",
        "\n",
        "print(f\"Dataset size: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset information\n",
        "print(\"Dataset information:\")\n",
        "print(df.info())\n",
        "print(\"\\nStatistics:\")\n",
        "print(df.describe())\n",
        "print(\"\\nTarget variable distribution:\")\n",
        "print(df['target'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target variable\n",
        "X = df.drop(columns='target')\n",
        "y = df['target']\n",
        "\n",
        "# Split into train/validation/test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Validation set size: {X_val.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "\n",
        "# Feature normalization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nNormalization completed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save data in CSV format for SageMaker\n",
        "# SageMaker expects data in format: target,feature1,feature2,...\n",
        "train_data = pd.concat([y_train.reset_index(drop=True), pd.DataFrame(X_train_scaled)], axis=1)\n",
        "val_data = pd.concat([y_val.reset_index(drop=True), pd.DataFrame(X_val_scaled)], axis=1)\n",
        "test_data = pd.concat([y_test.reset_index(drop=True), pd.DataFrame(X_test_scaled)], axis=1)\n",
        "\n",
        "# Save without headers and indices\n",
        "train_data.to_csv('train.csv', index=False, header=False)\n",
        "val_data.to_csv('validation.csv', index=False, header=False)\n",
        "test_data.to_csv('test.csv', index=False, header=False)\n",
        "\n",
        "print(\"Data saved for SageMaker:\")\n",
        "print(f\"- train.csv: {train_data.shape}\")\n",
        "print(f\"- validation.csv: {val_data.shape}\")\n",
        "print(f\"- test.csv: {test_data.shape}\")\n",
        "\n",
        "# Show first few rows\n",
        "print(\"\\nFirst rows of training data:\")\n",
        "print(train_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. SageMaker Setup\n",
        "\n",
        "Configure SageMaker session and upload data to S3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure SageMaker\n",
        "session = sagemaker.Session()\n",
        "bucket = session.default_bucket()\n",
        "prefix = \"heart-disease-prediction\"\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "print(f\"S3 Bucket: {bucket}\")\n",
        "print(f\"Prefix: {prefix}\")\n",
        "print(f\"Role: {role}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload data to S3\n",
        "train_path = session.upload_data(path='train.csv', bucket=bucket, key_prefix=f'{prefix}/data')\n",
        "val_path = session.upload_data(path='validation.csv', bucket=bucket, key_prefix=f'{prefix}/data')\n",
        "test_path = session.upload_data(path='test.csv', bucket=bucket, key_prefix=f'{prefix}/data')\n",
        "\n",
        "print(f\"Training data uploaded to: {train_path}\")\n",
        "print(f\"Validation data uploaded to: {val_path}\")\n",
        "print(f\"Test data uploaded to: {test_path}\")\n",
        "\n",
        "# Check upload\n",
        "!aws s3 ls {bucket}/{prefix}/data --recursive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Training Script\n",
        "\n",
        "Create a script for training the model in SageMaker.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training script\n",
        "training_script = '''\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import argparse\n",
        "import joblib\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import json\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    \"\"\"Load model from file\"\"\"\n",
        "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
        "    return model\n",
        "\n",
        "def input_fn(request_body, request_content_type):\n",
        "    \"\"\"Parse input data\"\"\"\n",
        "    if request_content_type == 'text/csv':\n",
        "        # For CSV data\n",
        "        data = pd.read_csv(io.StringIO(request_body), header=None)\n",
        "        return data.values\n",
        "    elif request_content_type == 'application/json':\n",
        "        # For JSON data\n",
        "        data = json.loads(request_body)\n",
        "        return np.array(data)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    \"\"\"Make predictions using the model\"\"\"\n",
        "    predictions = model.predict(input_data)\n",
        "    return predictions\n",
        "\n",
        "def output_fn(prediction, content_type):\n",
        "    \"\"\"Format output data\"\"\"\n",
        "    if content_type == 'application/json':\n",
        "        return json.dumps(prediction.tolist())\n",
        "    elif content_type == 'text/csv':\n",
        "        return ','.join(map(str, prediction))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported content type: {content_type}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--n-estimators', type=int, default=100)\n",
        "    parser.add_argument('--learning-rate', type=float, default=1.0)\n",
        "    parser.add_argument('--max-depth', type=int, default=1)\n",
        "    parser.add_argument('--random-state', type=int, default=42)\n",
        "    \n",
        "    # SageMaker arguments\n",
        "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
        "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
        "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
        "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    train_data = pd.read_csv(os.path.join(args.train, 'train.csv'), header=None)\n",
        "    val_data = pd.read_csv(os.path.join(args.validation, 'validation.csv'), header=None)\n",
        "    \n",
        "    # Split into features and target variable\n",
        "    X_train = train_data.iloc[:, 1:].values\n",
        "    y_train = train_data.iloc[:, 0].values\n",
        "    X_val = val_data.iloc[:, 1:].values\n",
        "    y_val = val_data.iloc[:, 0].values\n",
        "    \n",
        "    print(f\"Training set size: {X_train.shape}\")\n",
        "    print(f\"Validation set size: {X_val.shape}\")\n",
        "    \n",
        "    # Create and train model\n",
        "    print(\"Training model...\")\n",
        "    base_estimator = DecisionTreeClassifier(max_depth=args.max_depth, random_state=args.random_state)\n",
        "    model = AdaBoostClassifier(\n",
        "        base_estimator=base_estimator,\n",
        "        n_estimators=args.n_estimators,\n",
        "        learning_rate=args.learning_rate,\n",
        "        random_state=args.random_state\n",
        "    )\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate model\n",
        "    train_pred = model.predict(X_train)\n",
        "    val_pred = model.predict(X_val)\n",
        "    \n",
        "    train_accuracy = accuracy_score(y_train, train_pred)\n",
        "    val_accuracy = accuracy_score(y_val, val_pred)\n",
        "    \n",
        "    print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
        "    \n",
        "    # Save model\n",
        "    print(\"Saving model...\")\n",
        "    model_path = os.path.join(args.model_dir, 'model.joblib')\n",
        "    joblib.dump(model, model_path)\n",
        "    \n",
        "    print(\"Training completed successfully!\")\n",
        "'''\n",
        "\n",
        "# Save script\n",
        "with open('train_script.py', 'w') as f:\n",
        "    f.write(training_script)\n",
        "\n",
        "print(\"Training script created: train_script.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Model in SageMaker\n",
        "\n",
        "Create and run training job in SageMaker.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create SageMaker Estimator\n",
        "# Try these instance types if you get ResourceLimitExceeded error:\n",
        "# - ml.t3.medium (cheapest, good for small datasets)\n",
        "# - ml.t3.large (slightly more powerful)\n",
        "# - ml.m5.large (if you have quota for it)\n",
        "# - ml.c5.large (compute optimized)\n",
        "\n",
        "sklearn_estimator = SKLearn(\n",
        "    entry_point='train_script.py',\n",
        "    framework_version='1.0-1',\n",
        "    py_version='py3',\n",
        "    instance_type='ml.m4.xlarge',  # Changed to supported instance type\n",
        "    instance_count=1,\n",
        "    role=role,\n",
        "    sagemaker_session=session,\n",
        "    hyperparameters={\n",
        "        'n-estimators': 100,\n",
        "        'learning-rate': 1.0,\n",
        "        'max-depth': 1,\n",
        "        'random-state': 42\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"SageMaker Estimator created\")\n",
        "print(f\"Instance type: ml.m4.xlarge\")\n",
        "print(f\"Instance count: 1\")\n",
        "print(\"\\nIf you get ResourceLimitExceeded error, try changing instance_type to:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting model training...\")\n",
        "sklearn_estimator.fit({\n",
        "    'train': train_path,\n",
        "    'validation': val_path\n",
        "}, wait=True)\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"Model saved to: {sklearn_estimator.model_data}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Deploy Model\n",
        "\n",
        "Deploy the trained model as an endpoint for predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy model\n",
        "print(\"Deploying model...\")\n",
        "predictor = sklearn_estimator.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type='ml.t3.medium',  # Changed from ml.t2.medium to supported type\n",
        "    serializer=CSVSerializer(),\n",
        "    deserializer=JSONDeserializer()\n",
        ")\n",
        "\n",
        "print(\"Model deployed successfully!\")\n",
        "print(f\"Endpoint name: {predictor.endpoint_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Model\n",
        "\n",
        "Test the deployed model on test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test model\n",
        "print(\"Testing model...\")\n",
        "\n",
        "# Prepare test data (features only, without target variable)\n",
        "test_features = X_test_scaled[:10]  # Take first 10 samples for testing\n",
        "test_labels = y_test[:10].values\n",
        "\n",
        "print(f\"Testing on {len(test_features)} samples\")\n",
        "print(f\"True labels: {test_labels}\")\n",
        "\n",
        "# Get predictions\n",
        "predictions = predictor.predict(test_features)\n",
        "print(f\"Model predictions: {predictions}\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f\"Accuracy on test data: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed evaluation on all test data\n",
        "print(\"Detailed model evaluation...\")\n",
        "\n",
        "# Get predictions for all test data\n",
        "all_predictions = predictor.predict(X_test_scaled)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, all_predictions)\n",
        "print(f\"Overall accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(confusion_matrix(y_test, all_predictions))\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, all_predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Prediction Examples\n",
        "\n",
        "Show prediction examples for individual patients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction examples for individual patients\n",
        "print(\"Prediction examples:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Take several examples from test set\n",
        "for i in range(5):\n",
        "    patient_data = X_test_scaled[i:i+1]  # Single sample\n",
        "    true_label = y_test.iloc[i]\n",
        "    prediction = predictor.predict(patient_data)[0]\n",
        "    \n",
        "    print(f\"Patient {i+1}:\")\n",
        "    print(f\"  True diagnosis: {'Heart disease present' if true_label == 1 else 'No heart disease'}\")\n",
        "    print(f\"  Model prediction: {'Heart disease present' if prediction == 1 else 'No heart disease'}\")\n",
        "    print(f\"  Correct: {'✓' if true_label == prediction else '✗'}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Cleanup Resources\n",
        "\n",
        "⚠️ **IMPORTANT**: Don't forget to delete the endpoint after finishing work to avoid additional costs!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete endpoint (uncomment to execute)\n",
        "# predictor.delete_endpoint()\n",
        "# print(\"Endpoint deleted\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
